FROM apache/airflow:2.8.2

USER root

# Install Java and Spark client (for spark-submit command only)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    openjdk-17-jre-headless \
    wget \
    procps \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME (auto-detect for amd64/arm64)
# Detect JAVA_HOME at build time
RUN JAVA_HOME_DETECTED=$(dirname $(dirname $(readlink -f $(which java)))) && \
    echo "export JAVA_HOME=${JAVA_HOME_DETECTED}" >> /etc/profile && \
    echo "${JAVA_HOME_DETECTED}" > /tmp/java_home && \
    echo "Detected JAVA_HOME: ${JAVA_HOME_DETECTED}"

# Read the detected JAVA_HOME and set it as ENV
# We need to use a workaround since ENV can't use command substitution directly
RUN JAVA_HOME_VALUE=$(cat /tmp/java_home) && \
    echo "JAVA_HOME=${JAVA_HOME_VALUE}" >> /etc/environment

# Set JAVA_HOME - this will be the detected value from the file
# Default to amd64, but it will be overridden by /etc/environment
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Create a script that ensures JAVA_HOME is set correctly at runtime
# This script will be sourced by spark-submit
RUN echo '#!/bin/bash\n\
# Ensure JAVA_HOME is set correctly\n\
if [ -z "$JAVA_HOME" ] || [ ! -d "$JAVA_HOME" ] || [ ! -f "$JAVA_HOME/bin/java" ]; then\n\
    JAVA_HOME_NEW=$(dirname $(dirname $(readlink -f $(which java))))\n\
    export JAVA_HOME="${JAVA_HOME_NEW}"\n\
    export PATH="${JAVA_HOME}/bin:${PATH}"\n\
    echo "Auto-detected JAVA_HOME: ${JAVA_HOME}"\n\
fi' > /usr/local/bin/set-java-env.sh && \
    chmod +x /usr/local/bin/set-java-env.sh && \
    echo 'source /usr/local/bin/set-java-env.sh' >> /etc/profile

# Download Spark (client only - for spark-submit command)
ENV SPARK_VERSION=3.4.1
ENV SPARK_HOME=/opt/spark
ENV PATH="${SPARK_HOME}/bin:${PATH}"

RUN wget -q "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz" && \
    tar xzf "spark-${SPARK_VERSION}-bin-hadoop3.tgz" -C /opt && \
    mv "/opt/spark-${SPARK_VERSION}-bin-hadoop3" "${SPARK_HOME}" && \
    rm "spark-${SPARK_VERSION}-bin-hadoop3.tgz"

# Configure Spark to use the detected JAVA_HOME
RUN JAVA_HOME_VALUE=$(cat /tmp/java_home) && \
    echo "export JAVA_HOME=${JAVA_HOME_VALUE}" >> ${SPARK_HOME}/conf/spark-env.sh && \
    chmod +x ${SPARK_HOME}/conf/spark-env.sh

# Create a wrapper for spark-submit that ensures JAVA_HOME is set
RUN JAVA_HOME_VALUE=$(cat /tmp/java_home) && \
    mv ${SPARK_HOME}/bin/spark-submit ${SPARK_HOME}/bin/spark-submit.original && \
    echo '#!/bin/bash\n\
# Set JAVA_HOME before running spark-submit\n\
export JAVA_HOME="'${JAVA_HOME_VALUE}'"\n\
export PATH="${JAVA_HOME}/bin:${PATH}"\n\
exec "'${SPARK_HOME}'/bin/spark-submit.original" "$@"' > ${SPARK_HOME}/bin/spark-submit && \
    chmod +x ${SPARK_HOME}/bin/spark-submit

# Download AWS/S3 JARs for Spark S3A filesystem support (required for MinIO)
RUN cd ${SPARK_HOME}/jars && \
    wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar && \
    echo "S3A JARs downloaded successfully"

USER airflow

# Install Airflow Spark provider for SparkSubmitOperator
RUN pip install --no-cache-dir \
    apache-airflow-providers-apache-spark==4.5.0 \
    boto3==1.26.137 \
    pydantic==2.0.3 \
    pydantic-settings==2.0.3 \
    PyYAML==6.0
