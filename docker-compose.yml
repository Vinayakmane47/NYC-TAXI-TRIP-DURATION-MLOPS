services:
  # MinIO for S3-compatible storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"  # API endpoint
      - "9001:9001"  # Console UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - mlops-network
    restart: unless-stopped

  # MinIO Init - Creates buckets automatically
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: /bin/sh
    command:
      - -c
      - |
        echo "Waiting for MinIO to be ready..."
        until mc alias set myminio http://minio:9000 minioadmin minioadmin 2>/dev/null; do
          echo "Waiting for MinIO..."
          sleep 2
        done
        echo "Creating buckets..."
        mc mb myminio/dvc-storage --ignore-existing || true
        mc mb myminio/nyc-taxi-bronze --ignore-existing || true
        mc mb myminio/nyc-taxi-silver --ignore-existing || true
        mc mb myminio/nyc-taxi-gold --ignore-existing || true
        echo "All MinIO buckets created successfully!"
    networks:
      - mlops-network
    restart: "no"

  # Spark Master - Runs Spark jobs
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: nyc-taxi-spark:latest
    container_name: spark-master
    hostname: spark-master
    user: root
    ports:
      - "8081:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master port
    environment:
      # MinIO/S3 configuration
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_ENDPOINT_URL=http://minio:9000
      # Spark configuration for S3
      - SPARK_MASTER_HOST=spark-master
    volumes:
      - ./src:/opt/airflow/src
      - ./spark_jobs:/opt/airflow/spark_jobs
      - ./data:/opt/airflow/data
      - ./feature_store:/opt/airflow/feature_store
    command: >
      bash -c "
      /opt/spark/sbin/start-master.sh &&
      tail -f /opt/spark/logs/*
      "
    networks:
      - mlops-network
    restart: unless-stopped

  # Spark Worker - Executes Spark tasks
  spark-worker:
    image: nyc-taxi-spark:latest
    container_name: spark-worker
    user: root
    depends_on:
      - spark-master
    ports:
      - "8082:8081"  # Spark Worker UI
    environment:
      # MinIO/S3 configuration
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_ENDPOINT_URL=http://minio:9000
      # Spark worker configuration
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_CORES=2
    volumes:
      - ./src:/opt/airflow/src
      - ./spark_jobs:/opt/airflow/spark_jobs
      - ./data:/opt/airflow/data
      - ./feature_store:/opt/airflow/feature_store
    command: >
      bash -c "
      sleep 10 &&
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
      tail -f /opt/spark/logs/*
      "
    networks:
      - mlops-network
    restart: unless-stopped

  # Airflow Postgres Database
  postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      POSTGRES_INITDB_ARGS: "-E UTF8"
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - mlops-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Airflow Redis (for CeleryExecutor)
  redis:
    image: redis:7-alpine
    container_name: airflow-redis
    command: redis-server --appendonly yes --requirepass ""
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - mlops-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Airflow Webserver
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: nyc-taxi-airflow:latest
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      &airflow-common-env
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__API__ENABLE_UI: 'true'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      # Parallelism settings for performance optimization
      AIRFLOW__CORE__PARALLELISM: '20'
      AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: '12'
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: '1'
      AIRFLOW__CELERY__WORKER_CONCURRENCY: '4'
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: '30'
      AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT: '120'
      # Logging settings (suppress verbose logs)
      AIRFLOW__LOGGING__LOGGING_LEVEL: 'WARNING'
      AIRFLOW__CORE__LOGGING_LEVEL: 'WARNING'
      AIRFLOW__FAB_LOGGING_LEVEL: 'WARNING'
      # Spark connection (standalone cluster)
      AIRFLOW__CONNECTIONS__SPARK_DEFAULT: 'spark://spark-master:7077'
      # MinIO connection
      AIRFLOW__CONNECTIONS__MINIO_CONN: 's3://minioadmin:minioadmin@minio:9000'
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_ENDPOINT_URL: http://minio:9000
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mlops-network
    restart: unless-stopped
    user: "${AIRFLOW_UID:-50000}:0"

  # Airflow Scheduler
  airflow-scheduler:
    image: nyc-taxi-airflow:latest
    container_name: airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mlops-network
    restart: unless-stopped
    user: "${AIRFLOW_UID:-50000}:0"

  # Airflow Worker
  airflow-worker:
    image: nyc-taxi-airflow:latest
    container_name: airflow-worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./spark_jobs:/opt/airflow/spark_jobs
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
    command: celery worker
    deploy:
      resources:
        limits:
          memory: 14G  # Increased memory limit for Spark driver (2g) + Airflow overhead + buffer
        reservations:
          memory: 6G  # Minimum memory reservation
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mlops-network
    restart: unless-stopped
    user: "${AIRFLOW_UID:-50000}:0"

  # Airflow Init (runs once to initialize database)
  airflow-init:
    image: nyc-taxi-airflow:latest
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      _AIRFLOW_WWW_USER_FIRSTNAME: Airflow
      _AIRFLOW_WWW_USER_LASTNAME: Admin
      _AIRFLOW_WWW_USER_EMAIL: airflow@example.com
      _AIRFLOW_WWW_USER_ROLE: Admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
      - ./scripts:/opt/airflow/scripts
    command: version
    networks:
      - mlops-network
    user: "${AIRFLOW_UID:-50000}:0"

  # Spark SQL Thrift Server - Exposes Spark SQL via JDBC for BI tools
  # NOTE: Currently disabled due to Java 21 compatibility issues with Spark 3.4.1
  # To enable: 
  # 1. Modify Dockerfile.spark to use Java 17: change openjdk-21-jre-headless to openjdk-17-jre-headless
  # 2. Rebuild: docker compose build spark-master spark-worker
  # 3. Uncomment the service below
  #
  # spark-thrift-server:
  #   image: nyc-taxi-spark:latest
  #   container_name: spark-thrift-server
  #   user: root
  #   depends_on:
  #     - spark-master
  #     - minio
  #   ports:
  #     - "10000:10000"
  #   environment:
  #     - AWS_ACCESS_KEY_ID=minioadmin
  #     - AWS_SECRET_ACCESS_KEY=minioadmin
  #     - AWS_ENDPOINT_URL=http://minio:9000
  #     - SPARK_MASTER=spark://spark-master:7077
  #   volumes:
  #     - ./src:/opt/airflow/src
  #     - ./spark_jobs:/opt/airflow/spark_jobs
  #     - ./data:/opt/airflow/data
  #     - ./scripts:/opt/airflow/scripts
  #   command: >
  #     bash -c "
  #     /opt/spark/sbin/start-thriftserver.sh
  #     --master spark://spark-master:7077
  #     --conf spark.sql.warehouse.dir=s3a://nyc-taxi-gold/warehouse
  #     --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
  #     --conf spark.hadoop.fs.s3a.access.key=minioadmin
  #     --conf spark.hadoop.fs.s3a.secret.key=minioadmin
  #     --conf spark.hadoop.fs.s3a.path.style.access=true
  #     --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
  #     --hiveconf hive.server2.thrift.port=10000
  #     --hiveconf hive.server2.thrift.bind.host=0.0.0.0
  #     --driver-memory 2g
  #     --executor-memory 2g
  #     && tail -f /opt/spark/logs/*
  #     "
  #   networks:
  #     - mlops-network
  #   restart: unless-stopped

  # Superset PostgreSQL Database (separate from Airflow)
  superset-postgres:
    image: postgres:15
    container_name: superset-postgres
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
      POSTGRES_INITDB_ARGS: "-E UTF8"
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - superset_postgres_data:/var/lib/postgresql/data
    networks:
      - mlops-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Superset Init (runs once to initialize database)
  superset-init:
    build:
      context: .
      dockerfile: Dockerfile.superset
    image: nyc-taxi-superset:latest
    container_name: superset-init
    depends_on:
      superset-postgres:
        condition: service_healthy
    environment:
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-your-secret-key-change-in-production}
      SUPERSET_DB_HOST: superset-postgres
      SUPERSET_DB_PORT: 5432
      SUPERSET_DB_USER: superset
      SUPERSET_DB_PASSWORD: superset
      SUPERSET_DB_NAME: superset
    volumes:
      - ./superset/config/superset_config.py:/app/pythonpath/superset_config.py
      - ./superset/init_superset.sh:/app/init_superset.sh
      - ./superset/config/superset_home:/app/superset_home
    command: >
      bash -c "
      chmod +x /app/init_superset.sh &&
      /app/init_superset.sh
      "
    networks:
      - mlops-network
    restart: "no"

  # Apache Superset - BI Dashboard Tool
  superset:
    image: nyc-taxi-superset:latest
    container_name: superset
    depends_on:
      superset-init:
        condition: service_completed_successfully
      superset-postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8088:8088"
    environment:
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-your-secret-key-change-in-production}
      SUPERSET_DB_HOST: superset-postgres
      SUPERSET_DB_PORT: 5432
      SUPERSET_DB_USER: superset
      SUPERSET_DB_PASSWORD: superset
      SUPERSET_DB_NAME: superset
      REDIS_HOST: redis
      REDIS_PORT: 6379
    volumes:
      - ./superset/config/superset_config.py:/app/pythonpath/superset_config.py
      - ./superset/config/superset_home:/app/superset_home
      - ./superset/dashboards:/app/dashboards
    command: bash -c "superset db upgrade && superset init && gunicorn --bind 0.0.0.0:8088 --workers 4 --timeout 120 --limit-request-line 0 --limit-request-field_size 0 'superset.app:create_app()'"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - mlops-network
    restart: unless-stopped


volumes:
  minio_data:
    driver: local
  postgres_data:
    driver: local
  redis_data:
    driver: local
  superset_postgres_data:
    driver: local

networks:
  mlops-network:
    driver: bridge
