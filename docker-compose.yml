services:
  # MinIO for S3-compatible storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"  # API endpoint
      - "9001:9001"  # Console UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - mlops-network
    restart: unless-stopped

  # MinIO Init - Creates buckets automatically
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: /bin/sh
    command:
      - -c
      - |
        echo "Waiting for MinIO to be ready..."
        until mc alias set myminio http://minio:9000 minioadmin minioadmin 2>/dev/null; do
          echo "Waiting for MinIO..."
          sleep 2
        done
        echo "Creating buckets..."
        mc mb myminio/dvc-storage --ignore-existing || true
        mc mb myminio/nyc-taxi-bronze --ignore-existing || true
        mc mb myminio/nyc-taxi-silver --ignore-existing || true
        mc mb myminio/nyc-taxi-gold --ignore-existing || true
        echo "âœ… All MinIO buckets created successfully!"
    networks:
      - mlops-network
    restart: "no"

  # Spark Master - Runs Spark jobs
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: nyc-taxi-spark:latest
    container_name: spark-master
    hostname: spark-master
    user: root
    ports:
      - "8081:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master port
    environment:
      # MinIO/S3 configuration
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_ENDPOINT_URL=http://minio:9000
      # Spark configuration for S3
      - SPARK_MASTER_HOST=spark-master
    volumes:
      - ./src:/opt/airflow/src
      - ./spark_jobs:/opt/airflow/spark_jobs
      - ./data:/opt/airflow/data
      - ./feature_store:/opt/airflow/feature_store
    command: >
      bash -c "
      /opt/spark/sbin/start-master.sh &&
      tail -f /opt/spark/logs/*
      "
    networks:
      - mlops-network
    restart: unless-stopped

  # Spark Worker - Executes Spark tasks
  spark-worker:
    image: nyc-taxi-spark:latest
    container_name: spark-worker
    user: root
    depends_on:
      - spark-master
    ports:
      - "8082:8081"  # Spark Worker UI
    environment:
      # MinIO/S3 configuration
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_ENDPOINT_URL=http://minio:9000
      # Spark worker configuration
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_CORES=2
    volumes:
      - ./src:/opt/airflow/src
      - ./spark_jobs:/opt/airflow/spark_jobs
      - ./data:/opt/airflow/data
      - ./feature_store:/opt/airflow/feature_store
    command: >
      bash -c "
      sleep 10 &&
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
      tail -f /opt/spark/logs/*
      "
    networks:
      - mlops-network
    restart: unless-stopped

  # Airflow Postgres Database
  postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      POSTGRES_INITDB_ARGS: "-E UTF8"
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - mlops-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Airflow Redis (for CeleryExecutor)
  redis:
    image: redis:7-alpine
    container_name: airflow-redis
    command: redis-server --appendonly yes --requirepass ""
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - mlops-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Airflow Webserver
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: nyc-taxi-airflow:latest
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      &airflow-common-env
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__API__ENABLE_UI: 'true'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      # MinIO connection
      AIRFLOW__CONNECTIONS__MINIO_CONN: 's3://minioadmin:minioadmin@minio:9000'
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_ENDPOINT_URL: http://minio:9000
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mlops-network
    restart: unless-stopped
    user: "${AIRFLOW_UID:-50000}:0"

  # Airflow Scheduler
  airflow-scheduler:
    image: nyc-taxi-airflow:latest
    container_name: airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mlops-network
    restart: unless-stopped
    user: "${AIRFLOW_UID:-50000}:0"

  # Airflow Worker
  airflow-worker:
    image: nyc-taxi-airflow:latest
    container_name: airflow-worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./spark_jobs:/opt/airflow/spark_jobs
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
    command: celery worker
    deploy:
      resources:
        limits:
          memory: 14G  # Increased memory limit for Spark driver (2g) + Airflow overhead + buffer
        reservations:
          memory: 6G  # Minimum memory reservation
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mlops-network
    restart: unless-stopped
    user: "${AIRFLOW_UID:-50000}:0"

  # Airflow Init (runs once to initialize database)
  airflow-init:
    image: nyc-taxi-airflow:latest
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      _AIRFLOW_WWW_USER_FIRSTNAME: Airflow
      _AIRFLOW_WWW_USER_LASTNAME: Admin
      _AIRFLOW_WWW_USER_EMAIL: airflow@example.com
      _AIRFLOW_WWW_USER_ROLE: Admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./feature_store:/opt/airflow/feature_store
      - ./data:/opt/airflow/data
      - ./scripts:/opt/airflow/scripts
    command: version
    networks:
      - mlops-network
    user: "${AIRFLOW_UID:-50000}:0"

volumes:
  minio_data:
    driver: local
  postgres_data:
    driver: local
  redis_data:
    driver: local

networks:
  mlops-network:
    driver: bridge
